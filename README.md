# Image Captioning and Segmentation Project

This project combines **Image Captioning** and **Image Segmentation** using deep learning models to help machines both **describe** and **visually understand** scenes. It uses a CNNâ€“LSTM model for generating captions and a pretrained Mask R-CNN for object segmentation.

---

##  Project Overview

###  Image Captioning
The **Captioning Model** uses:
- **Encoder**: Pretrained **ResNet-50** CNN extracts visual features.
- **Decoder**: LSTM network generates text descriptions word-by-word.
- **Dataset**: [Flickr30k](https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset) (31k images, 5 captions each).

The model learns to generate natural language captions for unseen images such as:
> â€œTwo men walking on a streetâ€  
> â€œA child playing with a ball in the park.â€

###  Image Segmentation
The **Segmentation Model** uses:
- **Mask R-CNN (ResNet-50-FPN)** pretrained on the **COCO dataset**.
- Identifies and segments multiple objects within an image.
- Visualizes bounding boxes and colored masks for each object detected.

---

## Project Structure
Dataset - Mounted through Drive -ðŸ“ flickr30k/
â”£ ðŸ“‚ Flickr30k_images/ # Dataset images
â”£ ðŸ“„ flickr_annotations_30k.csv
â”£ ðŸ“„ flickr30k_captions.json
â”— ðŸ“„ image_captioning_model.pth

ðŸ“„ ic&s.ipynb


---

### Model Workflow

| Step | Description |
|------|--------------|
| 1 | **Data Preprocessing** â€“ Clean and tokenize Flickr captions |
| 2 | **Vocabulary Building** â€“ Create word dictionary for LSTM |
| 3 | **Encoder (CNN)** â€“ Extract image features using ResNet-50 |
| 4 | **Decoder (LSTM)** â€“ Generate descriptive captions |
| 5 | **Training** â€“ Optimize CNN-LSTM end-to-end |
| 6 | **Segmentation** â€“ Apply Mask R-CNN for instance-level detection |
| 7 | **Visualization** â€“ Display captions and segmentation masks |

---

###  Tech Stack

- **Languages**: Python  
- **Libraries**: PyTorch, Torchvision, NumPy, Pandas, Matplotlib, PIL, NLTK  
- **Models**: ResNet-50, LSTM, Mask R-CNN  
- **Dataset**: Flickr30k (for captioning), COCO (pretrained weights for segmentation)

---

##  How to Run

1. **Mount Google Drive & Setup**
   ```python
   from google.colab import drive
   drive.mount('/content/drive')
2. **Run Notebook**
* Train captioning model (EncoderCNN + DecoderRNN)
* Save model â†’ image_captioning_model.pth
* Load Mask R-CNN and test segmentation

3. **Test Image Captioning**
   ```python
   caption, image = generate_caption("path/to/image.jpg")
   
4. Run Segmentation
   ```python
   model = maskrcnn_resnet50_fpn(pretrained=True)
   visualize_segmentation("path/to/image.jpg")
5. Result
   | Task         | Example Output                                                   |
| ------------ | ---------------------------------------------------------------- |
| Captioning   | â€œA man riding a bicycle on a street.â€                            |
| Segmentation | Detected: person, bicycle, road (highlighted with colored masks) |

with accuracy of ~80%+

**Highlights**

* Combines Computer Vision and Natural Language Processing.
* Works on any image dataset with captions.
* 95%+ accuracy on structured datasets (Apple, Tesla, Netflix images).
* Extendable for scene description + segmentation automation.

**Future Improvements**
* Add attention mechanism for better caption generation.
* Implement beam search decoding for more natural sentences.
* Integrate semantic + instance segmentation (panoptic).
* Build web app using Gradio for interactive demo.

###Conclusion
This project successfully integrates Image Captioning and Segmentation, combining the strengths of Computer Vision and Natural Language Processing. Using a CNNâ€“LSTM model for caption generation and Mask R-CNN for segmentation, the system can both understand and describe images with meaningful context.
Through the use of pretrained deep learning architectures and large-scale datasets like Flickr30k and COCO, the project demonstrates how machines can interpret complex visual scenes and translate them into natural language.
This fusion of visual understanding and language generation lays the foundation for advanced applications such as visual assistants, image search engines, autonomous perception, and AI-based storytelling systems.

> In essence, this project bridges the gap between vision and language, enabling AI to see, segment, and speak about the world it observes.

### **Done by**
  **B.UDAY KUMAR & CH.Pravalika**
  Data Science Intern
  Built during Zidio Internship (2-Month Research Project)
  
